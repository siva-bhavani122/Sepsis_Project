{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trajectory Clustering with DTW"
      ],
      "metadata": {
        "id": "uTbdk7nK8-AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "op-qnHZw9DdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install dtw-python\n",
        "from dtw import *\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# move back one directory\n",
        "import os\n",
        "os.chdir('..')\n"
      ],
      "metadata": {
        "id": "etnOpOWP9DB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "8dKmSVue9H4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LjnY4mi38zZg"
      },
      "outputs": [],
      "source": [
        "#@title Functions\n",
        "# FUNCTIONS FOR TRAJECTORY CLUSTERING ANALYSIS\n",
        "# CODE AUTHORED BY: SHAWHIN TALEBI\n",
        "\n",
        "# IMPORT MODULES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# df2array3D\n",
        "\n",
        "# FUNCTION TO CONVERT PATIENT CLINICAL DATA STORED IN 2D PANDAS DATAFRAME TO 3D\n",
        "# NUMPY ARRAY (WITH AND WITHOUT FILL)\n",
        "\n",
        "# INPUTS\n",
        "#   - df = pandas dataframe with raw patient data\n",
        "#   - IDs = list-like object with unique patient IDs\n",
        "\n",
        "# OUTPUTS\n",
        "#   - data = 3D numpy array with forward/backawrd filled patient data where 1st\n",
        "#   index corresponds to the patient ID, 2nd to time, and 3rd to variable\n",
        "#   - hasCompletelyMissing = list with patient IDs with completely missing variables\n",
        "#   - data_unfilled = 3D numpy array with unfilled (has NaNs) patient data where\n",
        "#   1st index corresponds to the patient ID, 2nd to time, and 3rd to variable\n",
        "\n",
        "# DEPENDENCIES\n",
        "#   - numpy\n",
        "#   - pandas\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def df2array3D(df, IDs):\n",
        "\n",
        "    # initialize 3d data array\n",
        "    data = np.empty([len(IDs),8,5])\n",
        "    data_unfilled = np.empty([len(IDs),8,5])\n",
        "\n",
        "    # create list to store ID's which have completely missing variable values\n",
        "    hasCompletelyMissing = []\n",
        "\n",
        "    # intialize index\n",
        "    i = 0\n",
        "\n",
        "    # loop through patient IDs\n",
        "    for ID in IDs:\n",
        "\n",
        "        # create a temporary dataframe with data for ID\n",
        "        temp_df = df[df.index==ID]\n",
        "\n",
        "        # edge case: if time is duplicated keep first one\n",
        "        if len(temp_df.hours_from_adm) != len(temp_df.hours_from_adm.unique()):\n",
        "            bool = ~temp_df.hours_from_adm.duplicated()\n",
        "\n",
        "        # initialize a temporary numpy array with data for ID\n",
        "        temp_array = np.empty([8,5])\n",
        "        temp_array[:] = np.NaN\n",
        "\n",
        "        # add data to temporary numpy array with data for ID\n",
        "        temp_array[temp_df.hours_from_adm, :] = temp_df.iloc[:, 1:6].to_numpy()\n",
        "\n",
        "        # convert tempoary array to dataframe to perform backward/forward fill\n",
        "        df2=pd.DataFrame(temp_array)\n",
        "        df2=df2.fillna(method='ffill')\n",
        "        df2=df2.fillna(method='bfill')\n",
        "\n",
        "        # convert back to numpy array\n",
        "        temp_array_filled=df2.to_numpy()\n",
        "\n",
        "        # check for completely missing variables\n",
        "        if np.sum(np.isnan(temp_array_filled))>23:\n",
        "\n",
        "            # if completely missing add ID to list\n",
        "            hasCompletelyMissing.append(ID)\n",
        "\n",
        "        # store ID data in 3d data array\n",
        "        data[i,:,:] = temp_array_filled\n",
        "        data_unfilled[i,:,:] = temp_array\n",
        "\n",
        "        # update index\n",
        "        i = i + 1\n",
        "\n",
        "    return data, hasCompletelyMissing, data_unfilled\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# dropCompletelyMissing\n",
        "\n",
        "# FUNCTION TO DROP SAMPLES WHICH HAVE VARIABLE VALUES THAT ARE COMPLETELY MISSING\n",
        "# E.G. PATIENT HAS NO TEMPERATURE DATA OVER 24 HOURS\n",
        "\n",
        "# INPUTS\n",
        "#   - IDs = list-like object with unique patient IDs\n",
        "#   - hasCompletelyMissing = list with patient IDs with completely missing variables\n",
        "#   - data = 3D numpy array with patient data where 1st index corresponds to the\n",
        "#    patient ID, 2nd to time, and 3rd to variable\n",
        "\n",
        "# OUTPUTS\n",
        "#   - X_train = 3D numpy array with patient data where 1st index corresponds to\n",
        "#   the patient ID, 2nd to time, and 3rd to variable\n",
        "\n",
        "# DEPENDENCIES\n",
        "#   - numpy\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def dropCompletelyMissing(IDs, hasCompletelyMissing, data):\n",
        "    # get indicies correspinding to the IDs that have completely missing variable\n",
        "    # values\n",
        "    x,idrop,j = np.intersect1d(IDs,hasCompletelyMissing,return_indices=True)\n",
        "    # define set of all ID indicies\n",
        "    id_set = set(range(len(IDs)))\n",
        "    # define set of ID indicies to drop\n",
        "    drop_set = set(idrop)\n",
        "\n",
        "    # take difference of 2 sets\n",
        "    ikeep = id_set-drop_set\n",
        "\n",
        "    # define training data\n",
        "    X_train = data[list(ikeep),:,:]\n",
        "\n",
        "    return X_train, ikeep\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# standardize2Ddataframe\n",
        "# FUNCTION TO STANDARDIZE 3D NUMPY ARRAY\n",
        "\n",
        "# INPUTS\n",
        "#   - df = pandas dataframe with patient data indexed by patient ID and 0th\n",
        "#   column corresponds to time\n",
        "\n",
        "# OUTPUTS\n",
        "#   - df = pandas dataframe with patient data indexed by patient ID and 0th\n",
        "#   column corresponds to time, and each column is standardized\n",
        "\n",
        "# DEPENDENCIES\n",
        "#   - numpy\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def standardize2Ddataframe(df):\n",
        "    df.iloc[:,1:] = (df.iloc[:,1:] - np.nanmean(df.iloc[:,1:], 0))/np.nanstd(df.iloc[:,1:], 0)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "7bOK_pxw9OYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "is_train = True\n",
        "\n",
        "# Load Train vs Valid data\n",
        "if is_train:\n",
        "  df = pd.read_csv('/content/drive/My Drive/final_cohort_sepsis.csv')\n",
        "else:\n",
        "  df = pd.read_csv('/content/drive/My Drive/final_cohort_sepsis_validation.csv')\n",
        "\n",
        "IDs = df.id.unique()\n",
        "\n",
        "# restructure dataframe\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "df = df.set_index('id')\n",
        "\n",
        "#Choose to apply standard deviation cap\n",
        "apply_std_cap = False # No Cap being applied\n",
        "std_devs = 6\n",
        "\n",
        "cols = ['respiratoryrate', 'heartrate', 'systolicBP', 'diastolicBP', 'temperature']\n",
        "\n",
        "if apply_std_cap:\n",
        "  for i in cols:\n",
        "    df[i] = np.where(df[i] > std_devs,std_devs, df[i])\n",
        "    df[i] = np.where(df[i] < -std_devs,-std_devs, df[i])\n",
        "\n",
        "# make copy of data frame for later analysis\n",
        "df2 = df.copy()"
      ],
      "metadata": {
        "id": "_4BQFoh9-tbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardize variables with respect to entire dataset\n",
        "df = standardize2Ddataframe(df)\n",
        "\n",
        "\n",
        "# store data in 3D array\n",
        "data, hasCompletelyMissing, data_unfilled = df2array3D(df, IDs)\n",
        "\n",
        "# define training data set\n",
        "X_train, ikeep = dropCompletelyMissing(IDs, hasCompletelyMissing, data)\n",
        "X_train_unfilled, ikeep = dropCompletelyMissing(IDs, hasCompletelyMissing, data_unfilled)"
      ],
      "metadata": {
        "id": "fJjaHgcY_jrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine Data\n",
        "print(\"Original Data Shape: \" + str(data.shape))\n",
        "print(\"Prepared Data Shape: \" + str(X_train.shape))\n",
        "df"
      ],
      "metadata": {
        "id": "gBbDcGLa_kpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DTW"
      ],
      "metadata": {
        "id": "FgeASXIHBr7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define number of patients\\n\",\n",
        "numPatients = X_train.shape[0]\n",
        "    # initialize array to store patient distances\\n\",\n",
        "dtw_distances = np.zeros((numPatients,numPatients))\n",
        "\n",
        "\n",
        "# time computation\n",
        "import timeit\n",
        "start = timeit.default_timer() \n",
        "\n",
        "# compute distance between each patient using DTW\n",
        "for i in range(numPatients):\n",
        "  if i % 1000 == 0:\n",
        "    print(i)\n",
        "\n",
        "  for j in range(numPatients):      \n",
        "        # do not compute self-distances\n",
        "        if i==j:\n",
        "            continue\n",
        "            \n",
        "        # do not compute distance between same pair twice\n",
        "        if i>j:\n",
        "            continue\n",
        "        \n",
        "        # perform dtw algorithm\n",
        "        alignment = dtw(X_train[i,:,:], X_train[j,:,:], keep_internals=True, distance_only=True)\n",
        "        \n",
        "        # store distance in dtw_distances\n",
        "        dtw_distances[i,j] = alignment.distance\n",
        "        dtw_distances[j,i] = alignment.distance\n",
        "\n",
        "\n",
        "# print computation time\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start)        \n",
        "        \n",
        "dtw_distances.shape"
      ],
      "metadata": {
        "id": "wHRSAVRJBtn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DTW calculated distance matrix\n",
        "pd.DataFrame(dtw_distances).to_csv('/content/drive/MyDrive/Vader_Final_Cohort/DTW/dtw_distances_matrix.csv')"
      ],
      "metadata": {
        "id": "c-gSlGmTBwBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize distances\n",
        "dtw_norm = dtw_distances/np.max(dtw_distances)\n",
        "\n",
        "# transform distances into similarities\n",
        "dtw_similarities = 1 - dtw_distances\n",
        "\n",
        "dtw_norm = pd.DataFrame(dtw_norm)\n",
        "dtw_norm.to_csv('/content/drive/MyDrive/Vader_Final_Cohort/DTW/distances_normalized_matrix.csv')"
      ],
      "metadata": {
        "id": "v2-U7_whCDrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export DTW norm to Concensus Clustering Algorithm**"
      ],
      "metadata": {
        "id": "wT70jYzQCSo5"
      }
    }
  ]
}